{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d6df2cb",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "# ML for Healthcare with Weights & Biases: Interpretable Baselines and Clinical Evaluation\n",
    "\n",
    "We continue our focus on shifting from coding models to *understanding* them — how well they perform, how trustworthy their probabilities are, and how they behave across patient groups. Strenghtening the foundation for clinically aware ML practice.\n",
    "\n",
    "**Objective**  \n",
    "Build, track, and interpret models predicting in-hospital mortality in ICU patients using Weights & Biases (W&B).  \n",
    "This notebook transforms standard Machine Learning (ML) practice into an auditable, interpretable, and clinically meaningful workflow\n",
    "\n",
    "**You will learn**\n",
    "- How to track model training and evaluation runs using Weights & Biases  \n",
    "- How to interpret models and understand their calibration  \n",
    "- How to examine fairness and subgroup performance  \n",
    "- How to communicate model insights for healthcare decisions\n",
    "\n",
    "**Models**\n",
    "We’ll compare three interpretable baselines:\n",
    "1. **Logistic Regression**: simple linear reference, easy to explain  \n",
    "2. **Decision Tree (shallow)**: intuitive splits, visually transparent  \n",
    "3. **Random Forest**: robust ensemble, main focus for tuning and interpretability\n",
    "\n",
    "**Dataset**\n",
    "[PhysioNet Challenge 2012 dataset](https://physionet.org/content/challenge-2012/1.0.0/), containing clinical measurements, demographics, and the target:  \n",
    "`In-hospital_death` (binary: 1 = patient died during stay, 0 = survived).\n",
    "\n",
    "**Weights & Biases**\n",
    "Used to:\n",
    "- Track configurations, metrics, and plots  \n",
    "- Compare models and hyperparameters  \n",
    "- Log interpretability results (feature importance, calibration, subgroups)  \n",
    "- Support model transparency and documentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b201233",
   "metadata": {},
   "source": [
    "# 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca632bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: imports, reproducibility, and Weights & Biases initialization\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import joblib # Added for model saving\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "import wandb\n",
    "from wandb.plot import roc_curve as wandb_roc_curve\n",
    "from wandb.plot import pr_curve as wandb_pr_curve\n",
    "from wandb.plot import confusion_matrix as wandb_cm\n",
    "from wandb import Api as WandbApi # Added for sweep automation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, \n",
    "    average_precision_score, \n",
    "    brier_score_loss,\n",
    "    roc_curve, \n",
    "    precision_recall_curve,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40912e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined once here and used by all model baseline cells\n",
    "def calibration_table(y_true, y_prob, n_bins=10):\n",
    "    \"\"\"\n",
    "    Calculates calibration metrics (ECE) and returns a table.\n",
    "    It bins predictions, compares average predicted prob to actual positive rate,\n",
    "    and computes the weighted average error (ECE).\n",
    "    \"\"\"\n",
    "    if isinstance(y_true, pd.Series):\n",
    "        y_true = y_true.values # Convert from pandas to numpy if needed\n",
    "        \n",
    "    # Use pd.qcut to bin probabilities into `n_bins` groups (quantiles)\n",
    "    q = pd.qcut(y_prob, q=n_bins, duplicates=\"drop\")\n",
    "    dfb = pd.DataFrame({\"y_true\": y_true, \"y_prob\": y_prob, \"bin\": q})\n",
    "    \n",
    "    # Group by the bins and calculate:\n",
    "    agg = dfb.groupby(\"bin\", observed=False).agg(\n",
    "        mean_prob=(\"y_prob\", \"mean\"),     # The average *predicted risk* in this bin\n",
    "        observed_rate=(\"y_true\", \"mean\"), # The actual *mortality rate* in this bin\n",
    "        count=(\"y_true\", \"size\")          # How many patients are in this bin\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Convert bin (Interval object) to string for W&B Table\n",
    "    agg[\"bin\"] = agg[\"bin\"].astype(str)\n",
    "    \n",
    "    # Calculate ECE\n",
    "    weights = agg[\"count\"] / agg[\"count\"].sum()\n",
    "    ece = float(np.sum(weights * np.abs(agg[\"observed_rate\"] - agg[\"mean_prob\"])))\n",
    "    \n",
    "    return agg, ece # Return the table and the single ECE score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d986fefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wandb login\n",
    "# !wandb.login(relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4e94e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "WB_PROJECT = \"ml-healthcare-intro\"\n",
    "\n",
    "# wandb.login() # Uncomment if not logged in\n",
    "run = wandb.init(\n",
    "    project=WB_PROJECT, \n",
    "    job_type=\"data-exploration\", # More descriptive job_type\n",
    "    name=\"01-data-exploration\",  # Clean name for the UI\n",
    "    config={\n",
    "        \"seed\": SEED,\n",
    "        \"framework\": \"scikit-learn\",\n",
    "        \"dataset\": \"physionet2012_set_a\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Log environment metadata\n",
    "wandb.config.update({\n",
    "    \"python_version\": sys.version.split()[0],\n",
    "    \"pandas_version\": pd.__version__,\n",
    "    \"numpy_version\": np.__version__,\n",
    "}, allow_val_change=True)\n",
    "\n",
    "print(f\"Weights & Biases tracking URL: {run.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1ffd0e",
   "metadata": {},
   "source": [
    "# 3. Data loading and initial checks\n",
    "We will load the dataset, confirm the target, and log basic summaries to Weights & Biases. This lets us explore class balance, missingness, and a quick data preview directly in the dashboard.\n",
    "\n",
    "### Action Items\n",
    "- Open the W&B run link generated above\n",
    "- Inspect the `data_preview_table` to understand the features\n",
    "- Check the `class_balance_table` to confirm the target imbalance\n",
    "- Review `missingness_top30_table` to identify problematic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea8f066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load Data ---\n",
    "PATH = \"PhysionetChallenge2012-set-a.csv.gz\"\n",
    "\n",
    "# Simple check to ensure the data file exists before trying to load it\n",
    "if not os.path.exists(PATH):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Error: The data file was not found at '{PATH}'. \"\n",
    "        \"Please ensure the dataset is in the correct directory.\"\n",
    "    )\n",
    "\n",
    "ICU = pd.read_csv(PATH, compression=\"gzip\")\n",
    "\n",
    "TARGET = \"In-hospital_death\"\n",
    "ID_COL = \"recordid\" if \"recordid\" in ICU.columns else None\n",
    "\n",
    "if TARGET not in ICU.columns:\n",
    "    raise ValueError(f\"Target column '{TARGET}' not found in dataset\")\n",
    "\n",
    "# Ensure target is numeric and binary\n",
    "ICU[TARGET] = pd.to_numeric(ICU[TARGET], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "# Basic facts\n",
    "n_rows, n_cols = ICU.shape\n",
    "pos_rate = float(ICU[TARGET].mean())\n",
    "\n",
    "# Class balance table\n",
    "cb_series = ICU[TARGET].value_counts().sort_index()\n",
    "class_balance_tbl = wandb.Table(data=[[int(k), int(v), float(v / n_rows)] for k, v in cb_series.items()],\n",
    "                                columns=[\"label\", \"count\", \"fraction\"])\n",
    "\n",
    "# Missingness table (top 5)\n",
    "miss = ICU.isna().mean().sort_values(ascending=False)\n",
    "miss_top = miss.head(5).reset_index()\n",
    "miss_top.columns = [\"column\", \"missing_fraction\"]\n",
    "missing_tbl = wandb.Table(data=miss_top.values.tolist(), columns=list(miss_top.columns))\n",
    "\n",
    "# Data preview table (sample up to 5 rows for UI responsiveness)\n",
    "preview = ICU.sample(n=min(5, len(ICU)), random_state=SEED)\n",
    "preview_tbl = wandb.Table(dataframe=preview)\n",
    "\n",
    "wandb.log({\n",
    "    \"dataset_rows\": n_rows,\n",
    "    \"dataset_cols\": n_cols,\n",
    "    \"positive_rate\": pos_rate,\n",
    "    \"class_balance_table\": class_balance_tbl,\n",
    "    \"missingness_top5_table\": missing_tbl,\n",
    "    \"data_preview_table\": preview_tbl\n",
    "})\n",
    "\n",
    "print(f\"Loaded ICU with shape {ICU.shape} and positive rate {pos_rate:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e68fdbf",
   "metadata": {},
   "source": [
    "# 4. Simple preprocessing\n",
    "### Preprocessing and splits\n",
    "We will split the data into train, validation, and test sets, impute missing values and one-hot encode categorical variables, and log feature lists and split sizes to Weights & Biases for transparency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04bc083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: split data and prepare simple pipelines\n",
    "\n",
    "# Define all columns to drop: the target, the ID, and the leaking 'Survival' feature\n",
    "LEAKAGE_COL = \"Survival\"\n",
    "COLS_TO_DROP = [c for c in [TARGET, ID_COL, LEAKAGE_COL] if c in ICU.columns]\n",
    "\n",
    "# Drop ID column if present\n",
    "X = ICU.drop(columns=COLS_TO_DROP)\n",
    "y = ICU[TARGET]\n",
    "\n",
    "# Identify categorical and numeric columns\n",
    "cat_cols = [c for c in X.columns if X[c].dtype == \"object\"]\n",
    "num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "# Split data (60 percent train, 20 percent validation, 20 percent test)\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=SEED\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.25, stratify=y_train_full, random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# Define transformations\n",
    "num_transformer = SimpleImputer(strategy=\"median\")\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_transformer, num_cols),\n",
    "        (\"cat\", cat_transformer, cat_cols)\n",
    "    ],\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "preprocessor.set_output(transform=\"pandas\")\n",
    "\n",
    "# Fit and transform\n",
    "X_train_t = preprocessor.fit_transform(X_train)\n",
    "X_val_t = preprocessor.transform(X_val)\n",
    "X_test_t = preprocessor.transform(X_test)\n",
    "\n",
    "# Log split sizes\n",
    "wandb.log({\n",
    "    \"train_rows\": len(X_train),\n",
    "    \"val_rows\": len(X_val),\n",
    "    \"test_rows\": len(X_test),\n",
    "    \"n_num_features_raw\": len(num_cols),\n",
    "    \"n_cat_features_raw\": len(cat_cols),\n",
    "    \"n_features_transformed\": X_train_t.shape[1]\n",
    "})\n",
    "\n",
    "# Log feature lists as W&B Tables for inspectability\n",
    "num_tbl = wandb.Table(data=[[c, \"numeric\"] for c in num_cols], columns=[\"feature\", \"type\"])\n",
    "cat_tbl = wandb.Table(data=[[c, \"categorical\"] for c in cat_cols], columns=[\"feature\", \"type\"])\n",
    "wandb.log({\"feature_list_numeric\": num_tbl, \"feature_list_categorical\": cat_tbl})\n",
    "\n",
    "print(\"Preprocessing complete\")\n",
    "\n",
    "# Finish the data exploration run\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01940624",
   "metadata": {},
   "source": [
    "Use the feature tables and split sizes in Weights & Biases to verify preprocessing choices  \n",
    "All models next will consume the same transformed matrices for fair comparisons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6087c95",
   "metadata": {},
   "source": [
    "# 5. Logistic Regression: establishing a simple reference\n",
    "\n",
    "Before exploring complex models, it’s helpful to start with a simple and interpretable baseline. Logistic Regression gives a linear relationship between features and the log-odds of the outcome. Helping us understand whether more flexible models (like Random Forests) truly add value.\n",
    "\n",
    "1. We’ll train a Logistic Regression model, evaluate it on the validation and test sets\n",
    "2. Log all metrics to Weights & Biases to compare later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8328bc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression baseline with W&B's built-in plotting\n",
    "run = wandb.init(\n",
    "    project=WB_PROJECT,\n",
    "    job_type=\"baseline\",\n",
    "    name=\"02-baseline-logistic-regression\", # Add clean name\n",
    "    config={\"model_type\": \"logistic_regression\", \"seed\": SEED},\n",
    "    reinit=True,\n",
    ")\n",
    "\n",
    "# The 'calibration_table' function is now defined globally in Section 2\n",
    "\n",
    "# Train\n",
    "log_reg = LogisticRegression(max_iter=500, solver=\"liblinear\", random_state=SEED)\n",
    "log_reg.fit(X_train_t, y_train)\n",
    "\n",
    "# Predict probabilities\n",
    "y_val_prob = log_reg.predict_proba(X_val_t)[:, 1]\n",
    "y_test_prob = log_reg.predict_proba(X_test_t)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "val_auc = roc_auc_score(y_val, y_val_prob)\n",
    "val_pr  = average_precision_score(y_val, y_val_prob)\n",
    "val_brier = brier_score_loss(y_val, y_val_prob)\n",
    "test_auc = roc_auc_score(y_test, y_test_prob)\n",
    "test_pr  = average_precision_score(y_test, y_test_prob)\n",
    "test_brier = brier_score_loss(y_test, y_test_prob)\n",
    "\n",
    "wandb.log({\n",
    "    \"val_auc\": val_auc,\n",
    "    \"val_pr\": val_pr,\n",
    "    \"val_brier\": val_brier,\n",
    "    \"test_auc\": test_auc,\n",
    "    \"test_pr\": test_pr,\n",
    "    \"test_brier\": test_brier\n",
    "})\n",
    "\n",
    "# --- Log Calibration Metrics (using global helper) ---\n",
    "cal_val_tbl, val_ece = calibration_table(y_val, y_val_prob)\n",
    "cal_test_tbl, test_ece = calibration_table(y_test, y_test_prob)\n",
    "wandb.log({\n",
    "    \"val_ece\": val_ece, \"test_ece\": test_ece, # ECE stands for Expected Calibration Error\n",
    "    \"calibration_table_val\": wandb.Table(dataframe=cal_val_tbl),\n",
    "    \"calibration_table_test\": wandb.Table(dataframe=cal_test_tbl)\n",
    "})\n",
    "\n",
    "# Create the 2D probability array that wandb.plot expects\n",
    "y_val_probas_2d = np.stack([1.0 - y_val_prob, y_val_prob], axis=1)\n",
    "\n",
    "# Pass the 2D array and remove the 'labels' argument\n",
    "wandb.log({\n",
    "    \"roc_curve_val\": wandb_roc_curve(y_val.values, y_val_probas_2d),\n",
    "    \"pr_curve_val\": wandb_pr_curve(y_val.values, y_val_probas_2d)\n",
    "})\n",
    "\n",
    "# Coefficients for transparency\n",
    "coef_df = pd.DataFrame({\"feature\": X_train_t.columns, \"coefficient\": log_reg.coef_[0]})\n",
    "coef_tbl = wandb.Table(dataframe=coef_df.sort_values(\"coefficient\", ascending=False))\n",
    "wandb.log({\"log_reg_coefficients\": coef_tbl})\n",
    "\n",
    "# Predictions table sample\n",
    "pred_sample = pd.DataFrame({\n",
    "    \"id\": X_val.index if ID_COL is None else X_val.index, \n",
    "    \"y_true\": y_val.values,\n",
    "    \"y_prob\": y_val_prob\n",
    "}).sample(n=min(500, len(y_val)), random_state=SEED)\n",
    "wandb.log({\"predictions_val_sample\": wandb.Table(dataframe=pred_sample)})\n",
    "\n",
    "run.finish()\n",
    "\n",
    "print(f\"LR validation AUROC {val_auc:.3f}, AUPRC {val_pr:.3f}, Brier {val_brier:.3f}, ECE {val_ece:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42b057b",
   "metadata": {},
   "source": [
    "### To Do\n",
    "- Open the new \"02-baseline-logistic-regression\" run in W&B\n",
    "- Examine the interactive `roc_curve_val` and `pr_curve_val` plots\n",
    "- Sort the `log_reg_coefficients` table to find the strongest predictors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859adf44",
   "metadata": {},
   "source": [
    "# 6. Decision Tree baseline\n",
    "\n",
    "A shallow tree is easy to read and helps us see simple non-linear rules. We will train a small tree, log metrics, curve points, feature importances, and a predictions sample to Weights & Biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13c561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Decision Tree baseline\n",
    "run = wandb.init(\n",
    "    project=WB_PROJECT,\n",
    "    job_type=\"baseline\",\n",
    "    name=\"02-baseline-decision-tree\", # Add clean name\n",
    "    config={\n",
    "        \"model_type\": \"decision_tree\",\n",
    "        \"seed\": SEED,\n",
    "        \"max_depth\": 4,\n",
    "        \"min_samples_leaf\": 20\n",
    "    },\n",
    "    reinit=True,\n",
    ")\n",
    "\n",
    "# The 'calibration_table' function is now defined globally in Section 2\n",
    "\n",
    "# Train a small, readable tree\n",
    "dt = DecisionTreeClassifier(\n",
    "    max_depth=wandb.config.max_depth,\n",
    "    min_samples_leaf=wandb.config.min_samples_leaf,\n",
    "    random_state=SEED\n",
    ")\n",
    "dt.fit(X_train_t, y_train)\n",
    "\n",
    "# Predict probabilities\n",
    "y_val_prob = dt.predict_proba(X_val_t)[:, 1]\n",
    "y_test_prob = dt.predict_proba(X_test_t)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "val_auc = roc_auc_score(y_val, y_val_prob)\n",
    "val_pr  = average_precision_score(y_val, y_val_prob)\n",
    "val_brier = brier_score_loss(y_val, y_val_prob)\n",
    "test_auc = roc_auc_score(y_test, y_test_prob)\n",
    "test_pr  = average_precision_score(y_test, y_test_prob)\n",
    "test_brier = brier_score_loss(y_test, y_test_prob)\n",
    "\n",
    "wandb.log({\n",
    "    \"val_auc\": val_auc,\n",
    "    \"val_pr\": val_pr,\n",
    "    \"val_brier\": val_brier,\n",
    "    \"test_auc\": test_auc,\n",
    "    \"test_pr\": test_pr,\n",
    "    \"test_brier\": test_brier\n",
    "})\n",
    "\n",
    "# --- Log Calibration Metrics (using global helper) ---\n",
    "cal_val_tbl, val_ece = calibration_table(y_val, y_val_prob)\n",
    "cal_test_tbl, test_ece = calibration_table(y_test, y_test_prob)\n",
    "wandb.log({\n",
    "    \"val_ece\": val_ece, \"test_ece\": test_ece,\n",
    "    \"calibration_table_val\": wandb.Table(dataframe=cal_val_tbl),\n",
    "    \"calibration_table_test\": wandb.Table(dataframe=cal_test_tbl)\n",
    "})\n",
    "\n",
    "# Create the 2D probability array that wandb.plot expects\n",
    "y_val_probas_2d = np.stack([1.0 - y_val_prob, y_val_prob], axis=1)\n",
    "\n",
    "# Pass the 2D array and remove the 'labels' argument\n",
    "wandb.log({\n",
    "    \"roc_curve_val\": wandb_roc_curve(y_val.values, y_val_probas_2d),\n",
    "    \"pr_curve_val\": wandb_pr_curve(y_val.values, y_val_probas_2d)\n",
    "})\n",
    "\n",
    "# Feature importances\n",
    "imp_df = (\n",
    "    pd.DataFrame({\"feature\": X_train_t.columns, \"importance\": dt.feature_importances_})\n",
    "    .sort_values(\"importance\", ascending=False)\n",
    ")\n",
    "wandb.log({\"feature_importances\": wandb.Table(dataframe=imp_df)})\n",
    "\n",
    "# Predictions sample\n",
    "pred_sample = pd.DataFrame({\n",
    "    \"id\": X_val.index,\n",
    "    \"y_true\": y_val.values,\n",
    "    \"y_prob\": y_val_prob\n",
    "}).sample(n=min(500, len(y_val)), random_state=SEED)\n",
    "wandb.log({\"predictions_val_sample\": wandb.Table(dataframe=pred_sample)})\n",
    "\n",
    "run.finish()\n",
    "\n",
    "print(f\"DT validation AUROC {val_auc:.3f}, AUPRC {val_pr:.3f}, Brier {val_brier:.3f}, ECE {val_ece:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edb201b",
   "metadata": {},
   "source": [
    "# 7. Random Forest baseline\n",
    "### Random Forest baseline\n",
    "\n",
    "As we know, a Random Forest averages many trees to improve stability and performance. We will train a baseline model and log metrics, curve points, feature importances, and a predictions sample to W&B. Later we will tune hyperparameters with a short sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd8e099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Random Forest Comprehensive Baseline Analysis\n",
    "# This single cell trains one Random Forest model and then performs a\n",
    "# deep-dive analysis covering performance, calibration, thresholding, and fairness.\n",
    "\n",
    "# Start a new W&B run to log everything\n",
    "run = wandb.init(\n",
    "    project=WB_PROJECT,\n",
    "    job_type=\"baseline-comprehensive\", # A tag to group this run with other deep analyses\n",
    "    name=\"02-baseline-random-forest-full\", # A clear name for the W&B dashboard\n",
    "    config={\n",
    "        # --- Model Hyperparameters ---\n",
    "        \"model_type\": \"random_forest\",\n",
    "        \"seed\": SEED,\n",
    "        \"n_estimators\": 300,  # Number of trees in the forest\n",
    "        \"max_depth\": None,  # Let trees grow as deep as they want\n",
    "        \"max_features\": \"sqrt\", # Number of features to consider at each split\n",
    "        \"min_samples_leaf\": 5,  # Minimum samples required to be at a leaf node\n",
    "        \"n_jobs\": -1, # Use all available CPU cores for training\n",
    "        \n",
    "        # --- Analysis Parameters (for steps 3, 4, 5) ---\n",
    "        \"calibration_bins\": 10,       # How many bins to use for the ECE calculation\n",
    "        \"target_sensitivity\": 0.85, # A clinical goal: \"We must find at least 85% of mortality cases\"\n",
    "        \"target_specificity\": 0.90, # A clinical goal: \"We must correctly clear at least 90% of survival cases\"\n",
    "        \"subgroups\": [\"SOFA_bin\", \"CSRU\"] # Features to use for the fairness/bias check\n",
    "    },\n",
    "    reinit=True, # Allows running wandb.init() again in the same notebook\n",
    ")\n",
    "\n",
    "# --- 1. Train Model ---\n",
    "\n",
    "# Create a 'cfg' shortcut to access the config dictionary\n",
    "cfg = wandb.config \n",
    "\n",
    "# Initialize the RandomForestClassifier with hyperparameters from our config\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=cfg.n_estimators,\n",
    "    max_depth=cfg.max_depth,\n",
    "    max_features=cfg.max_features,\n",
    "    min_samples_leaf=cfg.min_samples_leaf,\n",
    "    random_state=SEED,\n",
    "    n_jobs=cfg.n_jobs\n",
    ")\n",
    "# Train (fit) the model on the training data\n",
    "rf.fit(X_train_t, y_train)\n",
    "\n",
    "# Get predicted probabilities for the positive class (mortality)\n",
    "# .predict_proba() returns [prob_of_0, prob_of_1], so [:, 1] selects just prob_of_1\n",
    "y_val_prob = rf.predict_proba(X_val_t)[:, 1]\n",
    "y_test_prob = rf.predict_proba(X_test_t)[:, 1]\n",
    "\n",
    "# --- 2. Log Baseline Metrics & Plots ---\n",
    "\n",
    "# Calculate standard performance metrics on both validation and test sets\n",
    "val_auc = roc_auc_score(y_val, y_val_prob)\n",
    "val_pr  = average_precision_score(y_val, y_val_prob) # AUPRC\n",
    "val_brier = brier_score_loss(y_val, y_val_prob)      # Mean Squared Error for probabilities\n",
    "\n",
    "test_auc = roc_auc_score(y_test, y_test_prob)\n",
    "test_pr  = average_precision_score(y_test, y_test_prob)\n",
    "test_brier = brier_score_loss(y_test, y_test_prob)\n",
    "\n",
    "# Group simple metrics into a dictionary for a single wandb.log() call\n",
    "metrics_log = {\n",
    "    \"val_auc\": val_auc, \"val_pr\": val_pr, \"val_brier\": val_brier,\n",
    "    \"test_auc\": test_auc, \"test_pr\": test_pr, \"test_brier\": test_brier\n",
    "}\n",
    "wandb.log(metrics_log)\n",
    "\n",
    "# W&B's built-in plotting functions (wandb_roc_curve, wandb_pr_curve)\n",
    "# expect a 2D array of probabilities: [prob_for_class_0, prob_for_class_1]\n",
    "y_val_probas_2d = np.stack([1.0 - y_val_prob, y_val_prob], axis=1)\n",
    "\n",
    "# Log the interactive plots to the W&B run\n",
    "wandb.log({\n",
    "    \"roc_curve_val\": wandb_roc_curve(y_val.values, y_val_probas_2d),\n",
    "    \"pr_curve_val\": wandb_pr_curve(y_val.values, y_val_probas_2d)\n",
    "})\n",
    "\n",
    "# Create a DataFrame of feature importances from the trained model\n",
    "imp_df = (\n",
    "    pd.DataFrame({\"feature\": X_train_t.columns, \"importance\": rf.feature_importances_})\n",
    "    .sort_values(\"importance\", ascending=False)\n",
    ")\n",
    "# Log this as an interactive W&B Table\n",
    "wandb.log({\"feature_importances\": wandb.Table(dataframe=imp_df)})\n",
    "\n",
    "# Create a DataFrame with a sample of predictions for manual inspection\n",
    "pred_sample = pd.DataFrame({\n",
    "    \"id\": X_val.index, \n",
    "    \"y_true\": y_val.values, \n",
    "    \"y_prob\": y_val_prob\n",
    "}).sample(n=min(500, len(y_val)), random_state=SEED) # Sample max 500 rows\n",
    "# Log this sample as a W&B Table\n",
    "wandb.log({\"predictions_val_sample\": wandb.Table(dataframe=pred_sample)})\n",
    "\n",
    "print(f\"RF validation AUROC {val_auc:.3f}, AUPRC {val_pr:.3f}, Brier {val_brier:.3f}\")\n",
    "\n",
    "# --- 3. Calibration Analysis (Trustworthiness Check) ---\n",
    "\n",
    "# --- REFACTORED: Redundant helper function removed ---\n",
    "# The 'calibration_table' function is now defined globally in Section 2\n",
    "\n",
    "# Calculate calibration for both validation and test sets\n",
    "cal_val_tbl, val_ece = calibration_table(y_val, y_val_prob, n_bins=cfg.calibration_bins)\n",
    "cal_test_tbl, test_ece = calibration_table(y_test, y_test_prob, n_bins=cfg.calibration_bins)\n",
    "\n",
    "# Log the ECE scores and the full calibration tables to W&B\n",
    "wandb.log({\n",
    "    \"val_ece\": val_ece, \n",
    "    \"test_ece\": test_ece,\n",
    "    \"calibration_table_val\": wandb.Table(dataframe=cal_val_tbl),\n",
    "    \"calibration_table_test\": wandb.Table(dataframe=cal_test_tbl)\n",
    "})\n",
    "print(f\"RF validation ECE {val_ece:.3f}, Test ECE {test_ece:.3f}\")\n",
    "\n",
    "\n",
    "# --- 4. Threshold Selection (Finding Clinical Cutoffs) ---\n",
    "\n",
    "# Helper function to get detailed metrics for a *single* probability threshold\n",
    "def metrics_at_threshold(y_true, y_prob, thr):\n",
    "    \"\"\"Calculates confusion matrix metrics for a given probability threshold.\"\"\"\n",
    "    if isinstance(y_true, pd.Series):\n",
    "        y_true = y_true.values # Convert to numpy array\n",
    "        \n",
    "    # Convert probabilities to binary predictions (0 or 1) based on the threshold\n",
    "    y_pred = (y_prob >= thr).astype(int)\n",
    "    \n",
    "    # Calculate the confusion matrix components\n",
    "    # .ravel() flattens the 2x2 matrix into a 1D array [tn, fp, fn, tp]\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "    \n",
    "    # Calculate key clinical metrics. Handle division by zero if a class is empty.\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else np.nan # Recall\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "    ppv = tp / (tp + fp) if (tp + fp) > 0 else np.nan # Precision\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else np.nan\n",
    "    prevalence = (tp + fn) / (tp + tn + fp + fn)\n",
    "    \n",
    "    # Return all metrics as a dictionary\n",
    "    return dict(\n",
    "        threshold=float(thr), tp=int(tp), fp=int(fp), tn=int(tn), fn=int(fn),\n",
    "        sensitivity=float(sensitivity), specificity=float(specificity),\n",
    "        ppv=float(ppv), npv=float(npv), prevalence=float(prevalence)\n",
    "    )\n",
    "\n",
    "# --- Find Optimal Thresholds on Validation Set ---\n",
    "\n",
    "# Create a grid of 501 potential thresholds to test\n",
    "# We use quantiles of the validation probabilities to get a sensitive grid\n",
    "grid = np.unique(np.quantile(y_val_prob, q=np.linspace(0, 1, 501)))\n",
    "\n",
    "# Run the helper function for *every* threshold in the grid on the *validation* data\n",
    "val_rows = [metrics_at_threshold(y_val, y_val_prob, thr) for thr in grid]\n",
    "val_tbl = pd.DataFrame(val_rows) # Convert list of dictionaries to a DataFrame\n",
    "\n",
    "# Find the threshold that gets *closest* to our target sensitivity (from config)\n",
    "# .abs() finds the absolute difference\n",
    "# .argsort() finds the row index of the *smallest* difference\n",
    "# .iloc[0] selects that best row\n",
    "thr_for_sens = val_tbl.iloc[(val_tbl[\"sensitivity\"] - cfg.target_sensitivity).abs().argsort()].iloc[0][\"threshold\"]\n",
    "\n",
    "# Find the threshold that gets *closest* to our target specificity (from config)\n",
    "thr_for_spec = val_tbl.iloc[(val_tbl[\"specificity\"] - cfg.target_specificity).abs().argsort()].iloc[0][\"threshold\"]\n",
    "\n",
    "# Log these chosen thresholds back to the W&B run's config\n",
    "wandb.config.update({\n",
    "    \"chosen_threshold_sensitivity\": float(thr_for_sens),\n",
    "    \"chosen_threshold_specificity\": float(thr_for_spec)\n",
    "}, allow_val_change=True) # allow_val_change lets us add to an existing config\n",
    "\n",
    "# --- Apply Chosen Thresholds to Test Set ---\n",
    "\n",
    "# Now, use the thresholds we found on validation to evaluate the *test* set\n",
    "test_at_sens = metrics_at_threshold(y_test, y_test_prob, thr_for_sens)\n",
    "test_at_spec = metrics_at_threshold(y_test, y_test_prob, thr_for_spec)\n",
    "\n",
    "# Log the full table of all 501 thresholds (from validation) for review\n",
    "wandb.log({\"validation_threshold_sweep\": wandb.Table(dataframe=val_tbl[[\"threshold\",\"sensitivity\",\"specificity\",\"ppv\",\"npv\",\"prevalence\"]])})\n",
    "\n",
    "# Create a small DataFrame summarizing the test set performance at our chosen points\n",
    "test_results_df = pd.DataFrame([\n",
    "    dict(target=\"sensitivity\", **test_at_sens), # \"**\" unpacks the dictionary\n",
    "    dict(target=\"specificity\", **test_at_spec)\n",
    "])\n",
    "# Log this summary table\n",
    "wandb.log({\"test_operating_points\": wandb.Table(dataframe=test_results_df)})\n",
    "\n",
    "# --- Log Interactive Confusion Matrices ---\n",
    "# Create binary predictions for the test set using our two chosen thresholds\n",
    "y_pred_sens = (y_test_prob >= thr_for_sens).astype(int)\n",
    "y_pred_spec = (y_test_prob >= thr_for_spec).astype(int)\n",
    "\n",
    "# Log interactive confusion matrix plots to W&B\n",
    "wandb.log({\n",
    "    \"confusion_matrix_test_at_sensitivity\": wandb_cm(y_true=y_test.values, preds=y_pred_sens, class_names=[\"Survived\", \"Died\"]),\n",
    "    \"confusion_matrix_test_at_specificity\": wandb_cm(y_true=y_test.values, preds=y_pred_spec, class_names=[\"Survived\", \"Died\"])\n",
    "})\n",
    "print(f\"Chosen thresholds -> Sensitivity target: {thr_for_sens:.3f}, Specificity target: {thr_for_spec:.3f}\")\n",
    "\n",
    "# --- 5. Subgroup Performance (Fairness & Bias Check) ---\n",
    "\n",
    "# Define the original column names for our subgroups\n",
    "SOFA_COL = \"SOFA\" # A clinical score for patient sickness\n",
    "CSRU_COL = \"CSRU\" # A binary flag for a specific ICU type\n",
    "\n",
    "# Get the original (non-preprocessed) subgroup features for the *test set* patients\n",
    "test_idx = X_test.index # Get the original index of the test set rows\n",
    "sofa_test = ICU.loc[test_idx, SOFA_COL] # Get SOFA scores for test patients\n",
    "csru_test = ICU.loc[test_idx, CSRU_COL] # Get CSRU status for test patients\n",
    "\n",
    "# Create the subgroup bins/categories\n",
    "# Bin SOFA scores into 5 quintiles (e.g., \"very low\", \"low\", \"medium\", \"high\", \"very high\")\n",
    "sofa_bins = pd.qcut(sofa_test, q=5, duplicates=\"drop\").astype(str)\n",
    "# Bin CSRU into \"CSRU\" vs \"non_CSRU\"\n",
    "csru_group = np.where(pd.to_numeric(csru_test, errors=\"coerce\").fillna(0).astype(int) == 1, \"CSRU\", \"non_CSRU\")\n",
    "\n",
    "# Helper function to get metrics for a subgroup (almost identical to the one in step 4)\n",
    "def subgroup_metrics_fixed_threshold(y_true, y_prob, thr):\n",
    "    \"\"\"Calculates confusion matrix metrics for a subgroup.\"\"\"\n",
    "    if isinstance(y_true, pd.Series):\n",
    "        y_true = y_true.values\n",
    "        \n",
    "    y_pred = (y_prob >= thr).astype(int)\n",
    "    \n",
    "    # We must handle cases where a small subgroup has 0 positive or 0 negative cases\n",
    "    # This would cause confusion_matrix() to error or return a non-4-element array\n",
    "    try:\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    except ValueError: \n",
    "        # Failsafe for empty or single-class slices\n",
    "        tn, fp, fn, tp = 0, 0, 0, 0\n",
    "        if len(y_pred) > 0:\n",
    "            # Manually calculate if possible\n",
    "            tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "            fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "            fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "            tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "\n",
    "    # Calculate metrics, checking for division by zero\n",
    "    sens = tp / (tp + fn) if (tp + fn) else np.nan\n",
    "    spec = tn / (tn + fp) if (tn + fp) else np.nan\n",
    "    ppv  = tp / (tp + fp) if (tp + fp) else np.nan\n",
    "    npv  = tn / (tn + fn) if (tn + fn) else np.nan\n",
    "    prev = (tp + fn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else np.nan\n",
    "    \n",
    "    return dict(\n",
    "        tp=int(tp), fp=int(fp), tn=int(tn), fn=int(fn),\n",
    "        sensitivity=float(sens), specificity=float(spec),\n",
    "        ppv=float(ppv), npv=float(npv), prevalence=float(prev)\n",
    "    )\n",
    "\n",
    "# This list will hold all results, one row per group per threshold\n",
    "subgroup_rows = []\n",
    "\n",
    "def add_group(group_name, group_values):\n",
    "    \"\"\"\n",
    "    Loops through all unique values in a group (e.g., all 5 SOFA bins),\n",
    "    calculates metrics for that slice, and appends to the subgroup_rows list.\n",
    "    \"\"\"\n",
    "    series = pd.Series(group_values, index=test_idx).astype(str)\n",
    "    \n",
    "    # Iterate over each unique value (e.g., \"CSRU\", then \"non_CSRU\")\n",
    "    for g in sorted(series.unique()):\n",
    "        \n",
    "        # Create a boolean mask to select only patients in this group\n",
    "        mask = (series == g).values\n",
    "        \n",
    "        # Slice the test set using the mask\n",
    "        y_true_g = y_test.values[mask] # True labels for this group\n",
    "        y_prob_g = y_test_prob[mask]  # Predictions for this group\n",
    "        \n",
    "        # Skip if the group is too small to calculate meaningful metrics\n",
    "        if len(y_true_g) < 10: continue\n",
    "            \n",
    "        if isinstance(y_true_g, pd.Series):\n",
    "            y_true_g = y_true_g.values\n",
    "        \n",
    "        # Calculate overall metrics (AUROC, AUPRC) for this subgroup\n",
    "        # Use try/except in case a subgroup has only 1 class (e.g., all survived)\n",
    "        try:\n",
    "            auroc = roc_auc_score(y_true_g, y_prob_g)\n",
    "        except ValueError:\n",
    "            auroc = np.nan\n",
    "        try:\n",
    "            auprc = average_precision_score(y_true_g, y_prob_g)\n",
    "        except ValueError:\n",
    "            auprc = np.nan\n",
    "            \n",
    "        # Calculate metrics at the *fixed thresholds* (found in step 4)\n",
    "        m_sens = subgroup_metrics_fixed_threshold(y_true_g, y_prob_g, thr_for_sens)\n",
    "        m_spec = subgroup_metrics_fixed_threshold(y_true_g, y_prob_g, thr_for_spec)\n",
    "        \n",
    "        # Add two rows to our results: one for each target threshold\n",
    "        subgroup_rows.append({\"subgroup_type\": group_name, \"subgroup_value\": g, \"n\": int(len(y_true_g)), \"auroc\": float(auroc), \"auprc\": float(auprc), \"target\": \"sensitivity\", \"threshold\": float(thr_for_sens), **m_sens})\n",
    "        subgroup_rows.append({\"subgroup_type\": group_name, \"subgroup_value\": g, \"n\": int(len(y_true_g)), \"auroc\": float(auroc), \"auprc\": float(auprc), \"target\": \"specificity\", \"threshold\": float(thr_for_spec), **m_spec})\n",
    "\n",
    "# Run the subgroup analysis function for our two defined groups\n",
    "add_group(\"SOFA_bin\", sofa_bins) # This will add ~10 rows (5 bins * 2 thresholds)\n",
    "add_group(\"ICU_unit\", csru_group) # This will add ~4 rows (2 bins * 2 thresholds)\n",
    "\n",
    "# Convert the final list of dictionaries into a DataFrame\n",
    "subgroup_df = pd.DataFrame(subgroup_rows)\n",
    "# Log the complete subgroup analysis as a W&B Table\n",
    "wandb.log({\"subgroup_metrics_test\": wandb.Table(dataframe=subgroup_df)})\n",
    "print(\"Logged subgroup metrics for SOFA bins and CSRU\")\n",
    "\n",
    "# --- 6. Finish Comprehensive Run ---\n",
    "# Mark the W&B run as complete\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7f4af3",
   "metadata": {},
   "source": [
    "### TO DO\n",
    "- Go to the W&B project page and compare the three `baseline` runs (LR, DT, RF)\n",
    "- Add `val_auc`, `val_pr`, and `val_brier` to the comparison table\n",
    "- Open the \"02-baseline-random-forest-full\" run. This run contains everything:\n",
    "    - **Metrics**: Check `val_pr` and `test_pr`\n",
    "    - **Plots**: Review the `roc_curve_val` and `calibration_table_val`\n",
    "    - **Thresholds**: Inspect the `test_operating_points` table and the `confusion_matrix...` plots\n",
    "    - **Fairness**: Analyze the `subgroup_metrics_test` table. How does `auroc` or `sensitivity` change between SOFA bins?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0195184a",
   "metadata": {},
   "source": [
    "# 8. Interpretability for understanding\n",
    "\n",
    "Model interpretability connects predictive performance to clinical meaning:\n",
    "- Use **Permutation Importance** for Logistic Regression, Decision Tree, and Random Forest  \n",
    "- Use **SHAP** on the Random Forest to see which features drive individual predictions  \n",
    "\n",
    "All outputs are logged to Weights & Biases for exploration and comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a501df48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretability: Permutation Importance and SHAP for Random Forest\n",
    "# Logs all interpretability outputs to W&B\n",
    "\n",
    "run = wandb.init(\n",
    "    project=WB_PROJECT,\n",
    "    job_type=\"interpretability\",\n",
    "    name=\"03-interpretability-report\", # Add clean name\n",
    "    config={\n",
    "        \"models\": [\"logistic_regression\", \"decision_tree\", \"random_forest\"],\n",
    "        \"shap_sample_size\": 500\n",
    "    },\n",
    "    reinit=True,\n",
    ")\n",
    "\n",
    "# 1) Permutation Importance for all models on Validation\n",
    "models = {\n",
    "    \"logistic_regression\": log_reg,\n",
    "    \"decision_tree\": dt,\n",
    "    \"random_forest\": rf\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    result = permutation_importance(\n",
    "        model, X_val_t, y_val, n_repeats=10, random_state=SEED, n_jobs=-1\n",
    "    )\n",
    "    imp_df = (\n",
    "        pd.DataFrame({\n",
    "            \"feature\": X_val_t.columns,\n",
    "            \"importance_mean\": result.importances_mean,\n",
    "            \"importance_std\": result.importances_std\n",
    "        })\n",
    "        .sort_values(\"importance_mean\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    wandb.log({f\"{model_name}_permutation_importance\": wandb.Table(dataframe=imp_df)})\n",
    "\n",
    "# 2) SHAP for Random Forest\n",
    "shap_sample_n = min(wandb.config.shap_sample_size, len(X_val_t))\n",
    "shap_sample = X_val_t.sample(n=shap_sample_n, random_state=SEED)\n",
    "\n",
    "explainer = shap.TreeExplainer(\n",
    "    rf,\n",
    "    model_output=\"raw\",\n",
    "    feature_perturbation=\"tree_path_dependent\"\n",
    ")\n",
    "\n",
    "# This logic handles inconsistencies in shap/sklearn versions\n",
    "shap_values_raw = explainer.shap_values(shap_sample, check_additivity=False)\n",
    "if isinstance(shap_values_raw, list):\n",
    "    sv = np.asarray(shap_values_raw[1])\n",
    "else:\n",
    "    sv = np.asarray(shap_values_raw)\n",
    "if sv.ndim == 3:\n",
    "    cls_axis = sv.shape[-1]\n",
    "    pick = 1 if cls_axis >= 2 else 0\n",
    "    sv = sv[..., pick]\n",
    "sv = np.squeeze(sv)\n",
    "assert sv.ndim == 2, f\"Expected 2D SHAP values, got {sv.shape}\"\n",
    "assert sv.shape[1] == shap_sample.shape[1], \"Feature count mismatch\"\n",
    "\n",
    "# Global summary plot\n",
    "shap.summary_plot(sv, shap_sample, show=False)\n",
    "wandb.log({\"shap_summary_plot\": wandb.Image(plt.gcf())})\n",
    "plt.close()\n",
    "\n",
    "# Ranked mean absolute SHAP for table\n",
    "mean_abs_shap = np.abs(sv).mean(axis=0).reshape(-1)\n",
    "feat_names = list(shap_sample.columns)\n",
    "shap_df = pd.DataFrame(\n",
    "    {\"feature\": feat_names, \"mean_abs_shap\": mean_abs_shap}\n",
    ").sort_values(\"mean_abs_shap\", ascending=False).reset_index(drop=True)\n",
    "wandb.log({\"rf_shap_feature_importance\": wandb.Table(dataframe=shap_df)})\n",
    "\n",
    "print(\"Logging SHAP dependence plots for top 2 features...\")\n",
    "top_features = shap_df[\"feature\"].head(2).tolist()\n",
    "\n",
    "for feature_name in top_features:\n",
    "    try:\n",
    "        fig, ax = plt.subplots()\n",
    "        shap.dependence_plot(feature_name, sv, shap_sample, ax=ax, show=False, interaction_index=None)\n",
    "        wandb.log({f\"shap_dependence_{feature_name}\": wandb.Image(fig)})\n",
    "        plt.close(fig)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not plot dependence for {feature_name}: {e}\")\n",
    "        plt.close('all') # Close any open figures to avoid bleed-over\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4cf119",
   "metadata": {},
   "source": [
    "### TO DO\n",
    "- In the `03-interpretability-report` run, compare the permutation importance tables across the three models\n",
    "- Examine the `shap_summary_plot` to see which features drive RF predictions (e.g., SOFA, CSRU)\n",
    "- Review the `shap_dependence_...` plots to understand *how* top features impact mortality risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d902532",
   "metadata": {},
   "source": [
    "# 9. Random Forest hyperparameter sweep\n",
    "\n",
    "We will run a short Weights & Biases Sweep to tune Random Forest hyperparameters with the goal if **Maximizing validation area under the Precision-Recall curve** \n",
    "- Remember Precision-Recall is more informative than ROC under class imbalance\n",
    "\n",
    "We log validation AUROC and Brier score as secondary signals for discrimination and calibration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73bb100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest sweep optimized for imbalanced data using validation AUPRC\n",
    "\n",
    "# Sweep training function\n",
    "def train_rf_sweep():\n",
    "    # Use job_type to group sweep agents\n",
    "    run = wandb.init(project=WB_PROJECT, job_type=\"sweep-agent\", reinit=True)\n",
    "    cfg = wandb.config\n",
    "\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=cfg.n_estimators,\n",
    "        max_depth=None if cfg.max_depth == 0 else cfg.max_depth,\n",
    "        max_features=cfg.max_features,\n",
    "        min_samples_leaf=cfg.min_samples_leaf,\n",
    "        class_weight=\"balanced\",   # encourage attention to minority class\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train_t, y_train)\n",
    "\n",
    "    # Validation probabilities\n",
    "    y_val_prob = model.predict_proba(X_val_t)[:, 1]\n",
    "\n",
    "    # Metrics\n",
    "    val_pr   = average_precision_score(y_val, y_val_prob)     # sweep objective\n",
    "    val_auc  = roc_auc_score(y_val, y_val_prob)\n",
    "    val_brier = brier_score_loss(y_val, y_val_prob)\n",
    "\n",
    "    # Log to W&B\n",
    "    wandb.log({\n",
    "        \"val_pr\": val_pr,\n",
    "        \"val_auc\": val_auc,\n",
    "        \"val_brier\": val_brier\n",
    "    })\n",
    "    run.finish()\n",
    "\n",
    "# Compact search space\n",
    "sweep_config = {\n",
    "    \"name\": \"rf_pr_tuning_v2\", # Give a new name\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\"name\": \"val_pr\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"n_estimators\": {\"values\": [100, 200, 300, 500]},\n",
    "        \"max_depth\": {\"values\": [0, 8, 12, 16]},          # 0 means None\n",
    "        \"max_features\": {\"values\": [\"sqrt\", \"log2\"]},\n",
    "        \"min_samples_leaf\": {\"values\": [1, 3, 5, 10]}\n",
    "    },\n",
    "    \"early_terminate\": {\n",
    "        \"type\": \"hyperband\",\n",
    "        \"min_iter\": 3\n",
    "    }\n",
    "}\n",
    "\n",
    "# Launch the sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=WB_PROJECT)\n",
    "print(f\"Sweep started. ID: {sweep_id}\")\n",
    "\n",
    "# Run 5 agents for the demo\n",
    "wandb.agent(sweep_id, function=train_rf_sweep, count=5)\n",
    "\n",
    "print(\"Sweep complete. In Weights & Biases, sort by val_pr and inspect Parameter Importance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd112b1d",
   "metadata": {},
   "source": [
    "### TO DO\n",
    "- Open the W&B Sweep link (e.g., `rf_pr_tuning_v2`)\n",
    "- View the Parallel Coordinates and Parameter Importance plots to see which hyperparameters matter most\n",
    "- Sort the sweep table by `val_pr` (descending) to find the best run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b646d44",
   "metadata": {},
   "source": [
    "# 10. Best model selection, Test evaluation, and Model Registry\n",
    "\n",
    "Let's pick the Random Forest configuration with the best validation Precision-Recall area from the sweep.\n",
    "- We will use the **W&B API** to programmatically fetch the best run's config\n",
    "- We refit that model on Train + Validation\n",
    "- We evaluate on Test\n",
    "- We log the final model as a **W&B Artifact** and register it in the **Model Registry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aae77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Random Forest evaluation using the best sweep config\n",
    "\n",
    "# --- 1. Fetch Best Run from Sweep ---\n",
    "print(\"Initializing W&B API to find best sweep run...\")\n",
    "api = WandbApi()\n",
    "\n",
    "# We need the full sweep path: f\"{entity}/{project}/{sweep_id}\"\n",
    "# We can get the entity by starting a temporary run\n",
    "try:\n",
    "    temp_run = wandb.init(project=WB_PROJECT, job_type=\"api_helper\", reinit=True)\n",
    "    ENTITY = temp_run.entity\n",
    "    temp_run.finish()\n",
    "    \n",
    "    sweep_path = f\"{ENTITY}/{WB_PROJECT}/{sweep_id}\"\n",
    "    print(f\"Accessing sweep at: {sweep_path}\")\n",
    "    sweep = api.sweep(sweep_path)\n",
    "    \n",
    "    best_run = sweep.best_run()\n",
    "    print(f\"Found best run: {best_run.name} with val_pr: {best_run.summary['val_pr']:.4f}\")\n",
    "\n",
    "    # --- 2. Get Best Parameters ---\n",
    "    best_params_config = best_run.config\n",
    "    \n",
    "    # Re-create the logic from the sweep function (e.g., max_depth=0 -> None)\n",
    "    rf_params = {\n",
    "        \"n_estimators\": best_params_config[\"n_estimators\"],\n",
    "        \"max_depth\": None if best_params_config[\"max_depth\"] == 0 else best_params_config[\"max_depth\"],\n",
    "        \"max_features\": best_params_config[\"max_features\"],\n",
    "        \"min_samples_leaf\": best_params_config[\"min_samples_leaf\"],\n",
    "        \"class_weight\": \"balanced\" # This was fixed in our sweep\n",
    "    }\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error fetching sweep data. Using fallback parameters. Error: {e}\")\n",
    "    # Fallback in case API fails in a restricted environment\n",
    "    rf_params = {\n",
    "        \"n_estimators\": 300, \"max_depth\": 12, \"max_features\": \"sqrt\",\n",
    "        \"min_samples_leaf\": 3, \"class_weight\": \"balanced\"\n",
    "    }\n",
    "    best_run = None # Flag that we used fallback\n",
    "\n",
    "# --- 3. Start Final Evaluation Run ---\n",
    "run = wandb.init(\n",
    "    project=WB_PROJECT,\n",
    "    job_type=\"final_eval\",\n",
    "    name=f\"04-final-model-{'fallback' if best_run is None else best_run.name}\",\n",
    "    config=rf_params, # Log the actual params used\n",
    "    reinit=True,\n",
    ")\n",
    "if best_run:\n",
    "    wandb.config.update({\"source_sweep\": sweep_path, \"source_run_id\": best_run.id})\n",
    "\n",
    "# --- REFACTORED: Redundant helper function removed ---\n",
    "# The 'calibration_table' function is now defined globally in Section 2\n",
    "\n",
    "# --- 4. Retrain Model on Train+Val ---\n",
    "print(\"Retraining best model on Train + Validation data...\")\n",
    "X_train_full_t = pd.concat([X_train_t, X_val_t])\n",
    "y_train_full = pd.concat([y_train, y_val])\n",
    "\n",
    "rf_best = RandomForestClassifier(\n",
    "    **rf_params, # Unpack the fetched params\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_best.fit(X_train_full_t, y_train_full)\n",
    "\n",
    "# --- 5. Evaluate and Log on Test ---\n",
    "y_test_prob = rf_best.predict_proba(X_test_t)[:, 1]\n",
    "\n",
    "test_pr = average_precision_score(y_test, y_test_prob)\n",
    "test_auc = roc_auc_score(y_test, y_test_prob)\n",
    "test_brier = brier_score_loss(y_test, y_test_prob)\n",
    "\n",
    "wandb.log({\n",
    "    \"test_pr\": test_pr,\n",
    "    \"test_auc\": test_auc,\n",
    "    \"test_brier\": test_brier\n",
    "})\n",
    "\n",
    "# --- ENHANCEMENT: Log Final Test Curves ---\n",
    "y_test_probas_2d = np.stack([1.0 - y_test_prob, y_test_prob], axis=1)\n",
    "wandb.log({\n",
    "    \"roc_curve_test_final\": wandb_roc_curve(y_test.values, y_test_probas_2d),\n",
    "    \"pr_curve_test_final\": wandb_pr_curve(y_test.values, y_test_probas_2d)\n",
    "})\n",
    "\n",
    "# Log calibration table and predictions\n",
    "cal_test_tbl, test_ece = calibration_table(y_test, y_test_prob)\n",
    "wandb.log({\n",
    "    \"test_ece_final\": test_ece,\n",
    "    \"calibration_table_test_final\": wandb.Table(dataframe=cal_test_tbl)\n",
    "})\n",
    "pred_tbl = pd.DataFrame({\"id\": X_test.index, \"y_true\": y_test.values, \"y_prob\": y_test_prob})\n",
    "wandb.log({\"final_test_predictions\": wandb.Table(dataframe=pred_tbl)})\n",
    "\n",
    "print(f\"Final Test AUROC {test_auc:.3f}, AUPRC {test_pr:.3f}, Brier {test_brier:.3f}, ECE {test_ece:.3f}\")\n",
    "\n",
    "\n",
    "print(\"Running subgroup (fairness) analysis on *final* tuned model...\")\n",
    "\n",
    "# --- Helper functions (copied from Cell 7) ---\n",
    "def subgroup_metrics_fixed_threshold(y_true, y_prob, thr):\n",
    "    if isinstance(y_true, pd.Series):\n",
    "        y_true = y_true.values\n",
    "    y_pred = (y_prob >= thr).astype(int)\n",
    "    try:\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    except ValueError: \n",
    "        tn, fp, fn, tp = 0, 0, 0, 0\n",
    "        if len(y_pred) > 0:\n",
    "            tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "            fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "            fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "            tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    sens = tp / (tp + fn) if (tp + fn) else np.nan\n",
    "    spec = tn / (tn + fp) if (tn + fp) else np.nan\n",
    "    ppv  = tp / (tp + fp) if (tp + fp) else np.nan\n",
    "    npv  = tn / (tn + fn) if (tn + fn) else np.nan\n",
    "    prev = (tp + fn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else np.nan\n",
    "    return dict(\n",
    "        tp=int(tp), fp=int(fp), tn=int(tn), fn=int(fn),\n",
    "        sensitivity=float(sens), specificity=float(spec),\n",
    "        ppv=float(ppv), npv=float(npv), prevalence=float(prev)\n",
    "    )\n",
    "\n",
    "subgroup_rows_final = [] # Use a new list name\n",
    "def add_group_final(group_name, group_values, y_true_all, y_prob_all, thr_sens, thr_spec):\n",
    "    series = pd.Series(group_values, index=y_true_all.index).astype(str)\n",
    "    for g in sorted(series.unique()):\n",
    "        mask = (series == g).values\n",
    "        y_true_g = y_true_all.values[mask]\n",
    "        y_prob_g = y_prob_all[mask]\n",
    "        if len(y_true_g) < 10: continue\n",
    "        if isinstance(y_true_g, pd.Series):\n",
    "            y_true_g = y_true_g.values\n",
    "        try:\n",
    "            auroc = roc_auc_score(y_true_g, y_prob_g)\n",
    "        except ValueError:\n",
    "            auroc = np.nan\n",
    "        try:\n",
    "            auprc = average_precision_score(y_true_g, y_prob_g)\n",
    "        except ValueError:\n",
    "            auprc = np.nan\n",
    "        m_sens = subgroup_metrics_fixed_threshold(y_true_g, y_prob_g, thr_sens)\n",
    "        m_spec = subgroup_metrics_fixed_threshold(y_true_g, y_prob_g, thr_spec)\n",
    "        subgroup_rows_final.append({\"subgroup_type\": group_name, \"subgroup_value\": g, \"n\": int(len(y_true_g)), \"auroc\": float(auroc), \"auprc\": float(auprc), \"target\": \"sensitivity\", \"threshold\": float(thr_sens), **m_sens})\n",
    "        subgroup_rows_final.append({\"subgroup_type\": group_name, \"subgroup_value\": g, \"n\": int(len(y_true_g)), \"auroc\": float(auroc), \"auprc\": float(auprc), \"target\": \"specificity\", \"threshold\": float(thr_spec), **m_spec})\n",
    "\n",
    "# --- Run Subgroup Analysis ---\n",
    "try:\n",
    "    _ = thr_for_sens\n",
    "    _ = thr_for_spec\n",
    "except NameError:\n",
    "    print(\"Warning: Thresholds from Cell 7 not found. Using default 0.5.\")\n",
    "    thr_for_sens = 0.5\n",
    "    thr_for_spec = 0.5\n",
    "\n",
    "# Get subgroup features (same as Cell 7)\n",
    "SOFA_COL = \"SOFA\"\n",
    "CSRU_COL = \"CSRU\"\n",
    "test_idx = X_test.index\n",
    "sofa_test = ICU.loc[test_idx, SOFA_COL]\n",
    "csru_test = ICU.loc[test_idx, CSRU_COL]\n",
    "sofa_bins = pd.qcut(sofa_test, q=5, duplicates=\"drop\").astype(str)\n",
    "csru_group = np.where(pd.to_numeric(csru_test, errors=\"coerce\").fillna(0).astype(int) == 1, \"CSRU\", \"non_CSRU\")\n",
    "\n",
    "# Run the analysis\n",
    "add_group_final(\"SOFA_bin\", sofa_bins, y_test, y_test_prob, thr_for_sens, thr_for_spec)\n",
    "add_group_final(\"ICU_unit\", csru_group, y_test, y_test_prob, thr_for_sens, thr_for_spec)\n",
    "\n",
    "# Log the final subgroup analysis as a W&B Table\n",
    "subgroup_df_final = pd.DataFrame(subgroup_rows_final)\n",
    "wandb.log({\"subgroup_metrics_test_FINAL\": wandb.Table(dataframe=subgroup_df_final)})\n",
    "print(\"Logged *final* subgroup metrics for SOFA bins and CSRU\")\n",
    "\n",
    "\n",
    "# --- 7. Log Model as Artifact and Register ---\n",
    "print(\"Logging model to W&B Artifacts and Model Registry...\")\n",
    "model_file = \"final_rf_model.joblib\"\n",
    "joblib.dump(rf_best, model_file)\n",
    "\n",
    "# Define the artifact\n",
    "model_at = wandb.Artifact(\n",
    "    \"best-rf-model\",\n",
    "    type=\"model\",\n",
    "    description=\"Final Random Forest model trained on train+val, tuned for AUPRC.\",\n",
    "    metadata=rf_params\n",
    ")\n",
    "model_at.add_file(model_file)\n",
    "\n",
    "# Log the artifact to the run\n",
    "run.log_artifact(model_at, aliases=[\"production_candidate\"])\n",
    "\n",
    "# Register the model in the Model Registry\n",
    "try:\n",
    "    run.link_artifact(model_at, f\"{WB_PROJECT}/ICU_Mortality_RF_Model\")\n",
    "    print(\"Successfully logged and registered model artifact\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: Could not auto-register model. Logged artifact instead. Error: {e}\")\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2536e19b",
   "metadata": {},
   "source": [
    "### TO DO\n",
    "- Open the `04-final-eval` run\n",
    "- Compare its `test_pr` metric against the `02-baseline-random-forest-full` run to quantify the impact of tuning\n",
    "- Go to the **Artifacts** tab in the W&B project (left-hand sidebar)\n",
    "- Find the `best-rf-model` artifact and inspect its contents and metadata\n",
    "- Go to the **Models** tab to see the registered `ICU_Mortality_RF_Model` and its version history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f8c091",
   "metadata": {},
   "source": [
    "# 11. Final Documentation: Model Card and Reporting\n",
    "\n",
    "The final step in a responsible ML workflow is documentation. This ensures that the model's performance, limitations, and intended use are understood by all stakeholders (e.g., clinicians, regulators, engineers).\n",
    "\n",
    "In Weights & Biases, you can create a **W&B Report** to weave together your findings into a comprehensive model card.\n",
    "\n",
    "### A good report for this project would include:\n",
    "\n",
    "-   **Objective**: The clinical goal (predicting in-hospital mortality)\n",
    "-   **Data**: A link to the `01-data-exploration` run, showing class balance and missingness\n",
    "-   **Models**: The \"Model Comparison\" table (from comparing `baseline` runs) showing why Random Forest was chosen\n",
    "-   **Tuning**: Key visualizations from the `rf_pr_tuning_v2` sweep\n",
    "-   **Final Performance**: Final metrics from the `04-final-eval` run (Test AUROC, AUPRC, Brier)\n",
    "-   **Interpretability**: The `shap_summary_plot` and `shap_dependence_...` plots from the `03-interpretability-report` run\n",
    "-   **Fairness & Safety**: The `subgroup_metrics_test` table, discussing performance on SOFA and CSRU groups\n",
    "-   **Clinical Use**: The `test_operating_points` table, explaining the trade-offs between the sensitivity and specificity thresholds\n",
    "\n",
    "You can also attach this information directly to the registered model in the W&B Model Registry UI. This creates a \"model card\" that lives with the model, ensuring downstream users understand its strengths and limitations before deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4664a395",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd598a80",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce20097c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icu-survival",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
